{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Problem Statement:*\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I used fashion mnist dataset for classification of multiclass. Here I used ANN architecture for the classification of images of fashion items with different styles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the more information about the dataset you can refer this: https://www.kaggle.com/zalando-research/fashionmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load required libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load fashion mnist dataset from the tensorflow.keras\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(xtrain,ytrain),(xtest,ytest)=fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape: (60000, 28, 28)\n",
      "ytrain shape: (60000,)\n",
      "xtest shape: (10000, 28, 28)\n",
      "ytest shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "#print shapes\n",
    "print('xtrain shape:',xtrain.shape)\n",
    "print('ytrain shape:',ytrain.shape)\n",
    "print('xtest shape:',xtest.shape)\n",
    "print('ytest shape:',ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here 60000 is samples,28*28 is shape of the each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "#print shape of first image \n",
    "print(xtrain[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 0 0 3 0]\n"
     ]
    }
   ],
   "source": [
    "#print first five ytrain values\n",
    "print(ytrain[:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see here that name of images are decoded as numbers. So, from the above mentioned link we can get corresponding names for classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class of this image is: Ankle boot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print any one image with the class number\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(xtrain[0,:,:])  #0: first sample\n",
    "print('Class of this image is:',class_names[ytrain[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape xtrain,xtest as row vector\n",
    "x_train=xtrain.reshape(60000,28*28)\n",
    "x_test=xtest.reshape(10000,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the ytrain,ytest \n",
    "y_train=tf.keras.utils.to_categorical(ytrain,num_classes=10)\n",
    "y_test=tf.keras.utils.to_categorical(ytest,num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#print y_train for checking the ytrain as encoded or not\n",
    "print(y_train)  #yes encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries for model building\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras import regularizers,optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(iterations,lr,lamda):\n",
    "    \n",
    "    #Define parameters\n",
    "    iterations=iterations\n",
    "    learning_rate=lr\n",
    "    hidden_nodes=512\n",
    "    output_nodes=10\n",
    "    \n",
    "    #Build model\n",
    "    model=Sequential()\n",
    "    #input_shape=28*28,units=number of hidden neurons required in first hidden layers\n",
    "    model.add(Dense(units=hidden_nodes,activation='relu',input_shape=(784,)))\n",
    "    #Define second hidden layers,here input_shape is not required becoz layer is sequential so it will remember last layers output nodes.\n",
    "    #those output nodes are becomes input_shape for the next layer\n",
    "    model.add(Dense(units=hidden_nodes,activation='relu'))\n",
    "    #Define third hidden layers\n",
    "    model.add(Dense(units=hidden_nodes,activation='relu'))\n",
    "    #Defne fourth hidden layer\n",
    "    model.add(Dense(units=hidden_nodes,activation='relu'))\n",
    "    #Define output layer,here activation function is softmax bcoz we have 10 classes in classification\n",
    "    #and introduce l2 regularization\n",
    "    model.add(Dense(units=output_nodes,activation='softmax',kernel_regularizer=regularizers.l2(lamda)))\n",
    "    \n",
    "    #Define optimizer\n",
    "    #first define sgd \n",
    "    sgd=optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer=sgd)\n",
    "    \n",
    "    #fit model\n",
    "    #I dont want to print the report at each batch(use verbose=0)  # want to see verbose=1\n",
    "    model.fit(x_train,y_train,epochs=iterations,batch_size=1000,verbose=0)\n",
    "    #print the report after running all epochs\n",
    "    [loss,train_score]=model.evaluate(x_train,y_train)\n",
    "    [loss,test_score]=model.evaluate(x_test,y_test)\n",
    "    return train_score,test_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.7180 - accuracy: 0.6366\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.7214 - accuracy: 0.6274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6365833282470703, 0.6273999810218811)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets give parameters and check \n",
    "lr=0.001\n",
    "lamda=0  # no regularization\n",
    "train_and_test_model(10,lr,lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.6010 - accuracy: 0.8001\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6196 - accuracy: 0.7871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.800083339214325, 0.7871000170707703)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now change the lr value to 0.01\n",
    "lr=0.01\n",
    "lamda=0  # no regularization\n",
    "train_and_test_model(10,lr,lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2232 - accuracy: 0.9178\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3241 - accuracy: 0.8800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9178333282470703, 0.8799999952316284)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now change the lr value to 0.1\n",
    "lr=0.1\n",
    "lamda=0  # no regularization\n",
    "train_and_test_model(50,lr,lamda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a some over fitting issue in the model,Because model is performance is good during train with an accuracy of 91.78%,But during testing 88% accuracy is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3798 - accuracy: 0.8724\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4459 - accuracy: 0.8531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8723833560943604, 0.8531000018119812)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #now give the lambda value\n",
    "lr=0.1\n",
    "lamda=0.01  #  regularization\n",
    "train_and_test_model(50,lr,lamda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the lambda value of .01 model is still facing overfitting problem and model with learning rate of 0.1 and lambda of 0.0 is giving the best result.So consider that model for future prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                              ----Thank you----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lamda=0.1:accuracy is 92.6%\n",
    "#lamda=0.01:accuracy is 96.66%\n",
    "#lamda=0.001:accuracy is 96.35%\n",
    "#lamda=0.0001:accuracy is 94.445%\n",
    "#increasing or decreasing of lambda value to greater extends,accuracy will get reduce so be caution with lambda value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(iterations,lr,lamda):\n",
    "    \n",
    "    #Define parameters\n",
    "    iterations=iterations\n",
    "    learning_rate=lr\n",
    "    hidden_nodes=256\n",
    "    output_nodes=10\n",
    "    \n",
    "    #Build model\n",
    "    model=Sequential()\n",
    "    #input_shape=28*28,units=number of hidden neurons required in first hidden layers\n",
    "    model.add(Dense(units=hidden_nodes,activation='relu',input_shape=(784,)))\n",
    "    #Define second hidden layers,here input_shape is not required becoz layer is sequential so it will remember last layers output nodes.\n",
    "    #those output nodes are becomes input_shape for the next layer\n",
    "    model.add(Dense(units=hidden_nodes,activation='relu'))\n",
    "    #Define output layer,here activation function is softmax bcoz we have 10 classes in classification\n",
    "    #and introduce l2 regularization\n",
    "    model.add(Dense(units=output_nodes,activation='softmax',kernel_regularizer=regularizers.l2(lamda)))\n",
    "    \n",
    "    #Define optimizer\n",
    "    #first define sgd \n",
    "    sgd=optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer=sgd)\n",
    "    \n",
    "    #fit model\n",
    "    #I dont want to print the report at each batch(use verbose=0)  # want to see verbose=1\n",
    "    model.fit(x_train,y_train,epochs=iterations,batch_size=1000,verbose=0)\n",
    "    #print the report after running all epochs\n",
    "    [loss,train_score]=model.evaluate(x_train,y_train)\n",
    "    [loss,test_score]=model.evaluate(x_test,y_test)\n",
    "    \n",
    "    return train_score,test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 1s 720us/step - loss: nan - accuracy: 0.1000\n",
      "313/313 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.10000000149011612, 0.10000000149011612)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=1\n",
    "lamda=0.01  #  regularization\n",
    "train_and_test_model(10,lr,lamda) #less overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Interpretation:*\n",
    "    \n",
    "   This model is free of overfitting issue because both train and test accuracy are same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 1s 660us/step - loss: nan - accuracy: 0.1000\n",
      "313/313 [==============================] - 1s 3ms/step - loss: nan - accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.10000000149011612, 0.10000000149011612)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=1\n",
    "lamda=10   #  regularization\n",
    "train_and_test_model(10,lr,lamda) # underfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This model is underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 1s 650us/step - loss: 2.2423 - accuracy: 0.1605\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.2402 - accuracy: 0.1595\n",
      "k: 1 epocs: 100 accuracy: (0.16054999828338623, 0.15950000286102295) alpha: 6.672576300767695e-05 Regularization: 2.667307323397617e-05\n",
      "1875/1875 [==============================] - 1s 702us/step - loss: 2.3396 - accuracy: 0.1287\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3418 - accuracy: 0.1292\n",
      "k: 2 epocs: 100 accuracy: (0.1287333369255066, 0.12919999659061432) alpha: 4.417517861072489e-07 Regularization: 3.368778329873847e-07\n",
      "1875/1875 [==============================] - 1s 661us/step - loss: 2.3417 - accuracy: 0.1049\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3416 - accuracy: 0.1051\n",
      "k: 3 epocs: 100 accuracy: (0.10490000247955322, 0.10509999841451645) alpha: 7.406044395134339e-06 Regularization: 4.1443042424509384e-05\n",
      "1875/1875 [==============================] - 1s 697us/step - loss: 2.3120 - accuracy: 0.1256\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3130 - accuracy: 0.1255\n",
      "k: 4 epocs: 100 accuracy: (0.1256166696548462, 0.12549999356269836) alpha: 3.025791648631914e-07 Regularization: 1.2096563723344393e-07\n",
      "1875/1875 [==============================] - 1s 683us/step - loss: 2.4325 - accuracy: 0.1025\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.4305 - accuracy: 0.1024\n",
      "k: 5 epocs: 100 accuracy: (0.10249999910593033, 0.10239999741315842) alpha: 1.3460881113703057e-06 Regularization: 0.0013797256418066038\n",
      "1875/1875 [==============================] - 1s 666us/step - loss: 0.5008 - accuracy: 0.8209\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.5322 - accuracy: 0.8071\n",
      "k: 6 epocs: 100 accuracy: (0.820900022983551, 0.8070999979972839) alpha: 0.0379148990904906 Regularization: 1.3190938502128778e-07\n",
      "1875/1875 [==============================] - 1s 668us/step - loss: 0.7763 - accuracy: 0.7455\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.7923 - accuracy: 0.7314\n",
      "k: 7 epocs: 100 accuracy: (0.7455000281333923, 0.7314000129699707) alpha: 0.00603329368178597 Regularization: 1.683279195049991e-06\n",
      "1875/1875 [==============================] - 1s 671us/step - loss: 0.4884 - accuracy: 0.8484\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.5202 - accuracy: 0.8337\n",
      "k: 8 epocs: 100 accuracy: (0.8484333157539368, 0.8337000012397766) alpha: 0.16436905894250814 Regularization: 0.007007855426115966\n",
      "1875/1875 [==============================] - 1s 646us/step - loss: 2.3216 - accuracy: 0.1477\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3211 - accuracy: 0.1473\n",
      "k: 9 epocs: 100 accuracy: (0.14773333072662354, 0.14730000495910645) alpha: 3.369336718223627e-05 Regularization: 0.004006790976403178\n"
     ]
    }
   ],
   "source": [
    "#Coarse tuning\n",
    "import math \n",
    "for k in range(1,10):\n",
    "    lr=math.pow(10,np.random.uniform(-7.0,3.0))\n",
    "    lamda=math.pow(10,np.random.uniform(-7,-2))\n",
    "\n",
    "    best_acc=train_and_test_model(10,lr,lamda)\n",
    "    \n",
    "    print('k:',k,'epocs:',100,'accuracy:',best_acc,'alpha:',lr,'Regularization:',lamda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Interpretation:*\n",
    "    \n",
    "   alpha=0.54,regularization=0.0001 for these values my model is giving the best result.\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some weights initializers are\n",
    "1. GLOROT Uniform  (keras uses this weight initializer)\n",
    "\n",
    "weigth_intializer=np.sqrt(3/(fan_avg))\n",
    "\n",
    "fan_avg=(fav_in+fan_out)/2\n",
    "\n",
    "2. He-normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "def tune_model(learning_rate,activation,dropout_rate,initializer,num_unit):\n",
    "\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Dense(units=num_unit,activation=activation,kernel_initializer=initializer,input_shape=(784,)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(units=num_unit,activation=activation,kernel_initializer=initializer))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(units=10,activation='softmax',kernel_regularizer=regularizers.l2(lamda)))\n",
    "    sgd=optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer=sgd)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=[20,50,100][:1]\n",
    "epochs=[1,20,50][:1]\n",
    "initializer=['lecun_uniform','normal','he_normal','he_uniform'][:1]\n",
    "learning_rate=[0.1,0.001,0.02][:1]\n",
    "dropout_rate=[0.3,0.2,0.8][:1]\n",
    "num_unit=[10,5][:1]\n",
    "activation=['relu','tanh','sigmoid','hard_sigmoid','linear'][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=dict(batch_size=batch_size,\n",
    "               epochs=epochs,\n",
    "               learning_rate=learning_rate,\n",
    "               dropout_rate=dropout_rate,\n",
    "               num_unit=num_unit,\n",
    "               initializer=initializer,\n",
    "               activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This package converts to model into scikit learn form\n",
    "model=tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=tune_model,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now model is converted to scikit learn form, so now we can apply grid search cv on it.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "models=GridSearchCV(estimator=model,param_grid=parameters,n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: {'activation': 'relu', 'batch_size': 20, 'dropout_rate': 0.3, 'epochs': 1, 'initializer': 'lecun_uniform', 'learning_rate': 0.1, 'num_unit': 10}\n"
     ]
    }
   ],
   "source": [
    "best_model=models.fit(x_train,y_train)\n",
    "print('Best model:',best_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
