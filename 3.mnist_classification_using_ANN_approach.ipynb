{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I used mnist dataset for classification of multiclass. Here I used ANN architecture for classification of images of numbers with different styles,And also discussed the effect of some hyperparamters such as learning rate,L2 regularizer and weight initializers on the model (Exploding Gradient and Vanishing Gradient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load required libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 8s 1us/step\n"
     ]
    }
   ],
   "source": [
    "#load mnist dataset from the tensorflow.keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(xtrain,ytrain),(xtest,ytest)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape: (60000, 28, 28)\n",
      "ytrain shape: (60000,)\n",
      "xtest shape: (10000, 28, 28)\n",
      "ytest shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "#print shapes\n",
    "print('xtrain shape:',xtrain.shape)\n",
    "print('ytrain shape:',ytrain.shape)\n",
    "print('xtest shape:',xtest.shape)\n",
    "print('ytest shape:',ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here 60000 is samples,28*28 is shape of the each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "#print shape of first image \n",
    "print(xtrain[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class of this image is: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(xtrain[0,:,:])\n",
    "print('Class of this image is:',ytrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape xtrain,xtest as row vector\n",
    "x_train=xtrain.reshape(60000,28*28)\n",
    "x_test=xtest.reshape(10000,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the ytrain,ytest \n",
    "y_train=tf.keras.utils.to_categorical(ytrain,num_classes=10)\n",
    "y_test=tf.keras.utils.to_categorical(ytest,num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#print y_train for checking the ytrain as encoded or not\n",
    "print(y_train)  #yes encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries for model building\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers,optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(iterations,lr,lamda):\n",
    "    \n",
    "    #Define parameters\n",
    "    iterations=iterations\n",
    "    learning_rate=lr\n",
    "    hidden_nodes=256\n",
    "    output_nodes=10\n",
    "    \n",
    "    #Build model\n",
    "    model=Sequential()\n",
    "    #input_shape=28*28,units=number of hidden neurons required in first hidden layers\n",
    "    model.add(Dense(units=hidden_nodes,activation='relu',input_shape=(784,)))\n",
    "    #Define second hidden layers,here input_shape is not required becoz layer is sequential so it will remember last layers output nodes.\n",
    "    #those output nodes are becomes input_shape for the next layer\n",
    "    model.add(Dense(units=hidden_nodes,activation='relu'))\n",
    "    #Define output layer,here activation function is softmax bcoz we have 10 classes in dataset\n",
    "    #and introduce l2 regularization\n",
    "    model.add(Dense(units=output_nodes,activation='softmax',kernel_regularizer=regularizers.l2(lamda)))\n",
    "    \n",
    "    #Define optimizer\n",
    "    #first define sgd \n",
    "    sgd=optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer=sgd)\n",
    "    \n",
    "    #fit model\n",
    "    #I dont want to print the report at each batch(use verbose=0)  # want to see verbose=1\n",
    "    model.fit(x_train,y_train,epochs=iterations,batch_size=1000,verbose=0)\n",
    "    #print the report after running all epochs\n",
    "    [loss,score]=model.evaluate(x_train,y_train)\n",
    "    \n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 1s 670us/step - loss: 2.2525 - accuracy: 0.1728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17284999787807465"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets give paramters values and check \n",
    "lr=0.0001\n",
    "lamda=0  # no regularization\n",
    "train_and_test_model(25,lr,lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for lr=0.0001 lamda=0 accuracy of the model is 17.28% for 25 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 1s 666us/step - loss: 1.4051 - accuracy: 0.7344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.734416663646698"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now change the lr value to 0.001\n",
    "lr=0.001\n",
    "lamda=0  # no regularization\n",
    "train_and_test_model(25,lr,lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr=0.001,lamda=0 accuracy of the model is 73.4% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 1s 692us/step - loss: 0.1130 - accuracy: 0.9686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9685666561126709"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now change the lr value to 0.1\n",
    "lr=0.1\n",
    "lamda=0  # no regularization\n",
    "train_and_test_model(25,lr,lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the reducing of learning rate model accuracy is increased to 96.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 1s 674us/step - loss: 0.3010 - accuracy: 0.9105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9104666709899902"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now change the lr value to 1\n",
    "lr=1\n",
    "lamda=0  # no regularization\n",
    "train_and_test_model(25,lr,lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy got dropped due to the increase of learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 1s 680us/step - loss: 2.3488 - accuracy: 0.0992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09920000284910202"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now change the lr value to 20\n",
    "lr=20\n",
    "lamda=0  # no regularization\n",
    "train_and_test_model(25,lr,lamda)   #this problems is called exploding gradient(loss is more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to the high learning rate of the model ,accuracy is not good and this problem is know as Exploding gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 2.3611 - accuracy: 0.0997\n",
      "Epoch 2/25\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 2.3601 - accuracy: 0.0992\n",
      "Epoch 3/25\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 2.3593 - accuracy: 0.0993\n",
      "Epoch 4/25\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 2.3617 - accuracy: 0.0988\n",
      "Epoch 5/25\n",
      "60/60 [==============================] - 1s 13ms/step - loss: 2.3581 - accuracy: 0.1004\n",
      "Epoch 6/25\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 2.3593 - accuracy: 0.1018\n",
      "Epoch 7/25\n",
      "60/60 [==============================] - 1s 13ms/step - loss: 2.3608 - accuracy: 0.0987\n",
      "Epoch 8/25\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 2.3601 - accuracy: 0.1009\n",
      "Epoch 9/25\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 2.3616 - accuracy: 0.0999\n",
      "Epoch 10/25\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 2.3601 - accuracy: 0.0992\n",
      "Epoch 11/25\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 2.3597 - accuracy: 0.1000\n",
      "Epoch 12/25\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 2.3591 - accuracy: 0.1006\n",
      "Epoch 13/25\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 2.3615 - accuracy: 0.1002\n",
      "Epoch 14/25\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 2.3622 - accuracy: 0.0981\n",
      "Epoch 15/25\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 2.3608 - accuracy: 0.0995\n",
      "Epoch 16/25\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 2.3623 - accuracy: 0.0987\n",
      "Epoch 17/25\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 2.3614 - accuracy: 0.0977 0s - loss: 2.3614 - accuracy: \n",
      "Epoch 18/25\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 2.3609 - accuracy: 0.0998\n",
      "Epoch 19/25\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 2.3605 - accuracy: 0.1001\n",
      "Epoch 20/25\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 2.3620 - accuracy: 0.0999\n",
      "Epoch 21/25\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 2.3628 - accuracy: 0.0980\n",
      "Epoch 22/25\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 2.3606 - accuracy: 0.0994\n",
      "Epoch 23/25\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 2.3599 - accuracy: 0.1004\n",
      "Epoch 24/25\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 2.3590 - accuracy: 0.1012\n",
      "Epoch 25/25\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 2.3615 - accuracy: 0.0988\n",
      "1875/1875 [==============================] - 2s 913us/step - loss: 2.3608 - accuracy: 0.0997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09973333030939102"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #now change the lr value to 0.0000000001\n",
    "lr=0.0000000001\n",
    "lamda=0  # no regularization\n",
    "train_and_test_model(25,lr,lamda)   #this problems is called vanishing gradient(loss is more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here is loss is remains same for all epochs.. this called vanishing gradient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2-regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 1s 675us/step - loss: 0.3980 - accuracy: 0.9171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9170500040054321"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now give the lambda value\n",
    "lr=0.1\n",
    "lamda=0.1  #  regularization\n",
    "train_and_test_model(25,lr,lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 2s 896us/step - loss: 0.7921 - accuracy: 0.8625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8624666929244995"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=0.1\n",
    "lamda=1  #  regularization\n",
    "train_and_test_model(25,lr,lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try different lambda values\n",
    "#lamda=0.1:accuracy is 91.7%\n",
    "#lamda=0.01:accuracy is 94.62%\n",
    "#lamda=0.001:accuracy is 96.51%\n",
    "#lamda=0.0001:accuracy is 96.93%\n",
    "#lamda=0.00001:accuracy is 96.78%\n",
    "#lamda=1:accuracy is 82.24%\n",
    "#increasing or decreasing of lambda value to a greater extends,accuracy will get reduce so be caution with lambda value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(iterations,lr,lamda):\n",
    "    \n",
    "    #Define parameters\n",
    "    iterations=iterations\n",
    "    learning_rate=lr\n",
    "    hidden_nodes=256\n",
    "    output_nodes=10\n",
    "    \n",
    "    #Build model\n",
    "    model=Sequential()\n",
    "    #input_shape=28*28,units=number of hidden neurons required in first hidden layers\n",
    "    model.add(Dense(units=hidden_nodes,activation='relu',input_shape=(784,)))\n",
    "    #Define second hidden layers,here input_shape is not required becoz layer is sequential so it will remember last layers output nodes.\n",
    "    #those output nodes are becomes input_shape for the next layer\n",
    "    model.add(Dense(units=hidden_nodes,activation='relu'))\n",
    "    #Define output layer,here activation function is softmax bcoz we have 10 classes in classification\n",
    "    #and introduce l2 regularization\n",
    "    model.add(Dense(units=output_nodes,activation='softmax',kernel_regularizer=regularizers.l2(lamda)))\n",
    "    \n",
    "    #Define optimizer\n",
    "    #first define sgd \n",
    "    sgd=optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer=sgd)\n",
    "    \n",
    "    #fit model\n",
    "    #If I dont want to print the report at each batch(use verbose=0)  # if you want to see verbose=1\n",
    "    model.fit(x_train,y_train,epochs=iterations,batch_size=1000,verbose=0)\n",
    "    #print the report after running all epochs\n",
    "    [loss,train_score]=model.evaluate(x_train,y_train)\n",
    "    [loss,test_score]=model.evaluate(x_test,y_test)\n",
    "    \n",
    "    return train_score,test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 1s 716us/step - loss: 0.1211 - accuracy: 0.9672\n",
      "313/313 [==============================] - 2s 1ms/step - loss: 0.1322 - accuracy: 0.9623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9671833515167236, 0.9623000025749207)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=0.1\n",
    "lamda=0.0001  #  regularization\n",
    "train_and_test_model(25,lr,lamda) #less overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Interpretation:*\n",
    "    \n",
    "   This model is free of overfitting issue because both train and test accuracy are same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 1s 723us/step - loss: 217.5611 - accuracy: 0.1316\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 217.5608 - accuracy: 0.1351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.13163332641124725, 0.13510000705718994)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=0.1\n",
    "lamda=10   #  regularization\n",
    "train_and_test_model(25,lr,lamda) # underfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This model is underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 1s 708us/step - loss: 0.1547 - accuracy: 0.9673\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1640 - accuracy: 0.9611\n",
      "k: 1 epocs: 100 accuracy: (0.9672999978065491, 0.9610999822616577) alpha: 0.2902043822654981 Regularization: 0.0014144062948781258\n",
      "1875/1875 [==============================] - 1s 715us/step - loss: 2.4218 - accuracy: 0.0684\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.4233 - accuracy: 0.0731\n",
      "k: 2 epocs: 100 accuracy: (0.06844999641180038, 0.0731000006198883) alpha: 1.1787137358188025e-05 Regularization: 2.716798682745542e-05\n",
      "1875/1875 [==============================] - 2s 841us/step - loss: 2.1957 - accuracy: 0.5170\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.1894 - accuracy: 0.5312\n",
      "k: 3 epocs: 100 accuracy: (0.5169666409492493, 0.5311999917030334) alpha: 0.0009173041145094651 Regularization: 0.009989352328536814\n",
      "1875/1875 [==============================] - 2s 875us/step - loss: 2.1400 - accuracy: 0.3727\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 2.1358 - accuracy: 0.3803\n",
      "k: 4 epocs: 100 accuracy: (0.3726833462715149, 0.38029998540878296) alpha: 0.0005455863401665111 Regularization: 1.0119515389150816e-06\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 1.9321 - accuracy: 0.5874\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 1.9220 - accuracy: 0.5987\n",
      "k: 5 epocs: 100 accuracy: (0.5874333381652832, 0.5986999869346619) alpha: 0.001226223684978512 Regularization: 0.000835211779115911\n",
      "1875/1875 [==============================] - 2s 752us/step - loss: 2.3061 - accuracy: 0.1080\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3098 - accuracy: 0.1079\n",
      "k: 6 epocs: 100 accuracy: (0.10803333669900894, 0.10790000110864639) alpha: 2.5753036342903603e-05 Regularization: 1.5339497199700382e-07\n",
      "1875/1875 [==============================] - 1s 709us/step - loss: 2.3272 - accuracy: 0.0874\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 2.3279 - accuracy: 0.0880\n",
      "k: 7 epocs: 100 accuracy: (0.08735000342130661, 0.08799999952316284) alpha: 3.060923188125463e-06 Regularization: 5.10632309189602e-06\n",
      "1875/1875 [==============================] - 1s 723us/step - loss: 2.3364 - accuracy: 0.1115\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 2.3381 - accuracy: 0.1103\n",
      "k: 8 epocs: 100 accuracy: (0.11145000159740448, 0.11029999703168869) alpha: 3.064831750413711e-06 Regularization: 0.0010775455249945646\n",
      "1875/1875 [==============================] - 2s 767us/step - loss: 0.2141 - accuracy: 0.9463\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2142 - accuracy: 0.9448\n",
      "k: 9 epocs: 100 accuracy: (0.946316659450531, 0.9448000192642212) alpha: 0.12331716260465292 Regularization: 0.0008832253998734603\n"
     ]
    }
   ],
   "source": [
    "#Coarse tuning\n",
    "import math \n",
    "for k in range(1,10):\n",
    "    lr=math.pow(10,np.random.uniform(-7.0,3.0))\n",
    "    lamda=math.pow(10,np.random.uniform(-7,-2))\n",
    "\n",
    "    best_acc=train_and_test_model(10,lr,lamda)\n",
    "    \n",
    "    print('k:',k,'epocs:',100,'accuracy:',best_acc,'alpha:',lr,'Regularization:',lamda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Interpretation:*\n",
    "    \n",
    "   alpha=0.29,regularization=0.0014144 for these values my model is giving the best result.\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Initailization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some weights initializers are\n",
    "1. GLOROT Uniform  (keras uses this weight initializer)\n",
    "\n",
    "weigth_intializer=np.sqrt(3/(fan_avg))\n",
    "\n",
    "fan_avg=(fav_in+fan_out)/2\n",
    "\n",
    "2. He-normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "def tune_model(learning_rate,activation,dropout_rate,initializer,num_unit):\n",
    "\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Dense(units=num_unit,activation=activation,kernel_initializer=initializer,input_shape=(784,)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(units=num_unit,activation=activation,kernel_initializer=initializer))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(units=10,activation='softmax',kernel_regularizer=regularizers.l2(lamda)))\n",
    "    sgd=optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer=sgd)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets define parameters\n",
    "batch_size=[20,50,100][:1]\n",
    "epochs=[1,20,50][:1]\n",
    "initializer=['lecun_uniform','normal','he_normal','he_uniform'][:1]\n",
    "learning_rate=[0.1,0.001,0.02][:1]\n",
    "dropout_rate=[0.3,0.2,0.8][:1]\n",
    "num_unit=[10,5][:1]\n",
    "activation=['relu','tanh','sigmoid','hard_sigmoid','linear'][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=dict(batch_size=batch_size,\n",
    "               epochs=epochs,\n",
    "               learning_rate=learning_rate,\n",
    "               dropout_rate=dropout_rate,\n",
    "               num_unit=num_unit,\n",
    "               initializer=initializer,\n",
    "               activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This package converts to model into scikit learn form\n",
    "model=tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=tune_model,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now model is converted to scikit learn form, so now we can apply grid search cv on it.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "models=GridSearchCV(estimator=model,param_grid=parameters,n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: {'activation': 'relu', 'batch_size': 20, 'dropout_rate': 0.3, 'epochs': 1, 'initializer': 'lecun_uniform', 'learning_rate': 0.1, 'num_unit': 10}\n"
     ]
    }
   ],
   "source": [
    "best_model=models.fit(x_train,y_train)\n",
    "print('Best model:',best_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8328666687011719"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For these values my model is giving 83.28%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                              ---Thank you---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
